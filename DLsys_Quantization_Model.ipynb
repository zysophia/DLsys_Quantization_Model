{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Post Training Quantization on Needle\n","\n","Fan Mo @cokespace2, Krish Parmar@parmar, Yue Zhuang@zysophia\n","\n","git: https://github.com/zysophia/DLsys_Quantization_Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1672837112784,"user":{"displayName":"yue zhuang","userId":"13156312362011556166"},"user_tz":480},"id":"xXR50aeznSA9","outputId":"29c64b3c-76d8-40d4-aebe-9a0b66e5d867"},"outputs":[],"source":["# Code to set up the assignment\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/\n","# !mkdir -p 10714\n","%cd /content/drive/MyDrive/10714\n","# !git clone https://github.com/zysophia/DLsys_Quantization_Model\n","%cd /content/drive/MyDrive/10714/DLsys_Quantization_Model\n","\n","!pip3 install pybind11"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1672837333416,"user":{"displayName":"yue zhuang","userId":"13156312362011556166"},"user_tz":480},"id":"DoxBIKWrpTRd"},"outputs":[],"source":["import sys\n","sys.path.append('./python')\n","sys.path.append('./apps')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!export CXX=/usr/bin/clang++ && make"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Overview of Quantization for Deep Learning\n","\n","\n","In deep learning, quantization refers to the process of reducing the precision of the weights and activations of a neural network model. Quantization helps reduce the size of a large deep learning model so that it can be stored and inferenced more efficiently on edge devices (eg: mobile phone) which have limited memory and computation resources. Typically, quantization uses fit number of bits (usually 8 or 16) to represent the values of the weight and activations of a model instead of using float point numbers.\n","\n","Quantization is often used along with other techniques such as pruning to further reduce the size and computational complexity of a model. There are mainly two types of quantizations in deep learning.\n","\n","1. Post-Training Quantization: Quantization is applied on a pre-trained model as the tensors are quantized to lower bit-width. There are multiple methods to transform the floating point to an int, including range mapping, scaling, etc. However, as we decrease the size of the model and lower the precision, there is no garantee on the performance on the model.\n","\n","2. Quantization Aware Training: The model is trained to be aware of quantization. The weights and activations are quantized and the gradients are computed using the quantized representation so that the model would be more robust to the quantization and will achieve a higher performance after that.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Summary of What We Have Achieved\n","\n","Considering the limit of the time, we will focus on implementing post training quantization in this project and leave the implementation of quantization-aware training to be done in the future. We will build our method on top of the needle library we have. \n","\n","* In this project, we were able to implement two methods `save_model()` and `load_model()` which dumps a trained model and read a trained model into memory.\n","\n","* We also implemented a method `quantize_model()` which takes a model as input and do the post training quantization on weights and activations. We compared the size of two models and the size of model is reduced significantly.\n","\n","* Finally, we evaluated the original model and quantized model on the testing data and observed comparable performance.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Code, Algorithms and Evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Save and Load Models\n","\n","We have implemented two methods `save_model()` and `load_model()` in tools/model_save_and_load.py. The method `load_named_params()` is called in `load_model()` as it updated the model's parameters with the given values recursively. We use pickle to dump and load the value dicts.\n","\n","Run the following block for tests on model saving/ loading."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 tests/test_quantize.py"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Fuse Convolution BatchNorm and ReLU\n","We implement the `fuse_conv_bn_relu()` method in tools/fuse_ops.py, which can fuse Convolution, BatchNorm and ReLU in one layer."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Quantization\n","\n","We implemented the `quantize_conv()` method in tools/quantize_weights.py. The method takes a nn.Conv layer as input and applied quantization by channel. It outputs the quantized data as well as the scales.\n","\n","Run the following block for tests on quantization."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 tools/quantize_test.py"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## References\n","\n","* https://iq.opengenus.org/basics-of-quantization-in-ml/\n","* https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNi6rz4DCKaGoJcexxpNL1s","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
